{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys import hopsworks_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project, fs = hopsworks_integration.feature_store.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hopsworks import udf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create hopsworks udf to convert from integer to float\n",
    "# @udf(return_type=[float, float], drop=[\"rating\", \"year_of_publication\"])\n",
    "# def int_to_float(rating: pd.Series, year_of_publication) -> pd.Series:\n",
    "#     return rating.astype(float), year_of_publication.astype(float)\n",
    "\n",
    "# # create hopsworks udf to convert from integer to string\n",
    "# @udf(return_type=str, drop=\"user_id\")\n",
    "# def int_to_str(user_id: pd.Series) -> pd.Series:\n",
    "#     return user_id.astype(str)\n",
    "# int_to_str.alias(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_fg = fs.get_feature_group(name=\"items\", version=1)\n",
    "users_fg = fs.get_feature_group(name=\"users\", version=1)\n",
    "ratings_fg = fs.get_feature_group(name=\"ratings\", version=1)\n",
    "\n",
    "selected_features = (\n",
    "    ratings_fg.select(\n",
    "        [\"user_id\", \"isbn\", \"rating\"]\n",
    "    )\n",
    "    .join(\n",
    "        users_fg.select([\"age\"]), #[\"location\", \"age\"]),\n",
    "        on=\"user_id\",\n",
    "        join_type=\"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        items_fg.select([\"year_of_publication\"]), #[\"book_title\", \"book_author\", \"year_of_publication\", \"publisher\"]),\n",
    "        on=\"isbn\",\n",
    "        join_type=\"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    version=2,\n",
    "    name=\"training_s\",\n",
    "    query=selected_features,\n",
    "    description=\"Two-Towers Training dataset\",\n",
    "    # transformation_functions = [\n",
    "    #     int_to_float(\"rating\", \"year_of_publication\").alias(\"rating\", \"year_of_publication\"),\n",
    "    #     int_to_str(\"user_id\").alias(\"user_id\"),\n",
    "    # ],\n",
    "    # labels = [\"rating\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(name=\"training_s\", version=2)\n",
    "# print(feature_view.read().show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debugging\n",
    "# X_train_df, X_val_df, X_test_df, y_train_df, y_val_df, y_test_df = (\n",
    "#             feature_view.train_validation_test_split(\n",
    "#                 validation_size=settings.TT_VALIDATION_SPLIT,\n",
    "#                 test_size=settings.TT_TEST_SPLIT,\n",
    "#                 description=\"Training dataset splits\",\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "# type(X_train_df)\n",
    "\n",
    "# X_train_df[\"year_of_publication\"].isna().sum()\n",
    "# print(f\"nan values in year_of_publication: {X_train_df['year_of_publication'].isna().sum()}\")\n",
    "# print(f\"nan values in age: {X_train_df['age'].isna().sum()}\")\n",
    "# print(f\"nan values in rating: {X_train_df['rating'].isna().sum()}\")\n",
    "\n",
    "# print(f\"fraction of nan values in year_of_publication: {X_train_df['year_of_publication'].isna().sum() / X_train_df.shape[0] :.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from recsys.config import settings\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(dict(settings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerDataset:\n",
    "    def __init__(self, feature_view, batch_size: int) -> None:\n",
    "        self._feature_view = feature_view\n",
    "        self._batch_size = batch_size\n",
    "        self._properties: dict | None\n",
    "\n",
    "    @property\n",
    "    def query_features(self) -> list[str]:\n",
    "        return [\"user_id\", \n",
    "                \"age\", \n",
    "                # \"location\"\n",
    "                ]\n",
    "\n",
    "    @property\n",
    "    def candidate_features(self) -> list[str]:\n",
    "        return [\n",
    "            \"isbn\",\n",
    "            # \"book_title\",\n",
    "            # \"book_author\",\n",
    "            \"year_of_publication\",\n",
    "            # \"publisher\",\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def properties(self) -> dict:\n",
    "        assert self._properties is not None, \"Call get_train_val_split() first.\"\n",
    "\n",
    "        return self._properties\n",
    "\n",
    "    def get_items_subset(self):\n",
    "        item_df = self.properties[\"X_train_df\"][self.candidate_features]\n",
    "        item_df.drop_duplicates(subset=\"isbn\", inplace=True)\n",
    "        item_ds = self.df_to_ds(item_df)\n",
    "\n",
    "        return item_ds\n",
    "\n",
    "    def get_train_val_split(self):\n",
    "        logger.info(\"Retrieving and creating train, val test split...\")\n",
    "\n",
    "        try:\n",
    "            X_train_df, X_val_df, X_test_df, y_train_df, y_val_df, y_test_df = (\n",
    "                self._feature_view.get_train_validation_test_split(1)\n",
    "            ) \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error: {e}\")\n",
    "            logger.info(\"Creating new train, val test split...\")\n",
    "            X_train_df, X_val_df, X_test_df, y_train_df, y_val_df, y_test_df = (\n",
    "                self._feature_view.train_validation_test_split(\n",
    "                    validation_size=settings.TT_VALIDATION_SPLIT,\n",
    "                    test_size=settings.TT_TEST_SPLIT,\n",
    "                    description=\"Training dataset splits\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        X_train_df[\"year_of_publication\"].isna().sum()\n",
    "        print(f\"nan values in year_of_publication: {X_train_df['year_of_publication'].isna().sum()}\")\n",
    "        print(f\"nan values in age: {X_train_df['age'].isna().sum()}\")\n",
    "        print(f\"nan values in rating: {X_train_df['rating'].isna().sum()}\")\n",
    "\n",
    "        train_ds = (\n",
    "            self.df_to_ds(X_train_df)\n",
    "            .batch(self._batch_size)\n",
    "            .cache()\n",
    "            .shuffle(self._batch_size * 10)\n",
    "        )\n",
    "        val_ds = self.df_to_ds(X_val_df).batch(self._batch_size).cache()\n",
    "\n",
    "        self._properties = {\n",
    "            \"X_train_df\": X_train_df,\n",
    "            \"X_val_df\": X_val_df,\n",
    "            \"query_df\": X_train_df[self.query_features],\n",
    "            \"item_df\": X_train_df[self.candidate_features],\n",
    "            \"user_ids\": X_train_df[\"user_id\"].unique().tolist(),\n",
    "            \"item_ids\": X_train_df[\"isbn\"].unique().tolist(),\n",
    "            # \"publisher\": X_train_df[\"publisher\"].unique().tolist(),\n",
    "            # \"book_author\": X_train_df[\"book_author\"].unique().tolist(),\n",
    "            # \"location\": X_train_df[\"location\"].unique().tolist(),\n",
    "            # \"book_title\": X_train_df[\"book_title\"].unique().tolist(),\n",
    "        }\n",
    "\n",
    "        return train_ds, val_ds\n",
    "\n",
    "    def df_to_ds(self, df):\n",
    "        return tf.data.Dataset.from_tensor_slices({col: df[col].to_list() for col in df})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TwoTowerDataset(\n",
    "    feature_view=feature_view, batch_size=settings.TT_BATCH_SIZE\n",
    ")\n",
    "train_ds, val_ds = dataset.get_train_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training samples: {len(dataset.properties['X_train_df']):,}\")\n",
    "logger.info(f\"Validation samples: {len(dataset.properties['X_val_df']):,}\")\n",
    "\n",
    "logger.info(f\"Number of users: {len(dataset.properties['user_ids']):,}\")\n",
    "logger.info(f\"Number of items: {len(dataset.properties['item_ids']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup, Embedding, Normalization, Dense, TextVectorization, GlobalAveragePooling1D\n",
    "from tensorflow.keras import regularizers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTowerFactory:\n",
    "    def __init__(self, dataset: \"TwoTowerDataset\") -> None:\n",
    "        self._dataset = dataset\n",
    "\n",
    "    def build(\n",
    "        self, embed_dim: int = settings.TT_EMBEDDING_DIM\n",
    "    ) -> \"QueryTower\":\n",
    "        return QueryTower(\n",
    "            user_ids=self._dataset.properties[\"user_ids\"],\n",
    "            emb_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "class QueryTower(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 user_ids: list, \n",
    "                 emb_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                StringLookup(vocabulary=user_ids, mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    # Add an additional embedding to account for unknown tokens.\n",
    "                    len(user_ids) + 1,\n",
    "                    emb_dim,\n",
    "                    embeddings_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.normalized_age = Normalization(axis=None)\n",
    "\n",
    "        self.fnn = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(\n",
    "                    emb_dim, \n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                    bias_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                    emb_dim,\n",
    "                    kernel_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                    bias_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        concatenated_inputs = tf.concat(\n",
    "            [\n",
    "                self.user_embedding(inputs[\"user_id\"]),\n",
    "                tf.reshape(self.normalized_age(inputs[\"age\"]), (-1, 1)),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        outputs = self.fnn(concatenated_inputs)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model_factory = QueryTowerFactory(dataset=dataset)\n",
    "query_model = query_model_factory.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTowerFactory:\n",
    "    def __init__(self, dataset: \"TwoTowerDataset\") -> None:\n",
    "        self._dataset = dataset\n",
    "\n",
    "    def build(\n",
    "        self, embed_dim: int = settings.TT_EMBEDDING_DIM\n",
    "    ) -> \"ItemTower\":\n",
    "        return ItemTower(\n",
    "            item_ids=self._dataset.properties[\"item_ids\"],\n",
    "            # garment_groups=self._dataset.properties[\"garment_groups\"],\n",
    "            # index_groups=self._dataset.properties[\"index_groups\"],\n",
    "            emb_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "\n",
    "class ItemTower(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        item_ids: list,\n",
    "        emb_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.item_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                StringLookup(vocabulary=item_ids, mask_token=None),\n",
    "                tf.keras.layers.Embedding(\n",
    "                    # Add an additional embedding to account for unknown tokens.\n",
    "                    len(item_ids) + 1,\n",
    "                    emb_dim,\n",
    "                    embeddings_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.normalized_year = Normalization(axis=None)\n",
    "\n",
    "        self.fnn = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(\n",
    "                    emb_dim, \n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                    bias_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                    emb_dim,\n",
    "                    kernel_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                    bias_regularizer=regularizers.l2(settings.TT_WEIGHT_DECAY),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        concatenated_inputs = tf.concat(\n",
    "            [\n",
    "                self.item_embedding(inputs[\"isbn\"]),\n",
    "                tf.reshape(self.normalized_year(inputs[\"year_of_publication\"]), (-1, 1)),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        outputs = self.fnn(concatenated_inputs)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_model_factory = ItemTowerFactory(dataset=dataset)\n",
    "item_model = item_model_factory.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerFactory:\n",
    "    def __init__(self, \n",
    "                 dataset: \"TwoTowerDataset\") -> None:\n",
    "        self._dataset = dataset\n",
    "\n",
    "    def build(\n",
    "        self,\n",
    "        query_model: QueryTower,\n",
    "        item_model: ItemTower,\n",
    "        batch_size: int = settings.TT_BATCH_SIZE,\n",
    "    ) -> \"TwoTowerModel\":\n",
    "        item_ds = self._dataset.get_items_subset()\n",
    "\n",
    "        return TwoTowerModel(\n",
    "            query_model,\n",
    "            item_model,\n",
    "            item_ds=item_ds,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "class TwoTowerModel(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_model: QueryTower,\n",
    "        item_model: ItemTower,\n",
    "        item_ds: tf.data.Dataset,\n",
    "        batch_size: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.query_model = query_model\n",
    "        self.item_model = item_model\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=item_ds.batch(batch_size).map(self.item_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train_step(self, batch) -> tf.Tensor:\n",
    "        # Set up a gradient tape to record gradients.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Loss computation.\n",
    "            user_embeddings = self.query_model(batch)\n",
    "            item_embeddings = self.item_model(batch)\n",
    "            loss = self.task(\n",
    "                user_embeddings,\n",
    "                item_embeddings,\n",
    "                compute_metrics=False,\n",
    "            )\n",
    "\n",
    "            # Handle regularization losses as well.\n",
    "            regularization_loss = sum(self.losses)\n",
    "            print(f\"{regularization_loss=}\")\n",
    "\n",
    "            total_loss = loss + regularization_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "            \"regularization_loss\": regularization_loss,\n",
    "            \"total_loss\": total_loss,\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch) -> tf.Tensor:\n",
    "        # Loss computation.\n",
    "        user_embeddings = self.query_model(batch)\n",
    "        item_embeddings = self.item_model(batch)\n",
    "\n",
    "        loss = self.task(\n",
    "            user_embeddings,\n",
    "            item_embeddings,\n",
    "            compute_metrics=False,\n",
    "        )\n",
    "\n",
    "        # Handle regularization losses as well.\n",
    "        regularization_loss = sum(self.losses)\n",
    "\n",
    "        total_loss = loss + regularization_loss\n",
    "\n",
    "        metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "        metrics[\"loss\"] = loss\n",
    "        metrics[\"regularization_loss\"] = regularization_loss\n",
    "        metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = TwoTowerFactory(dataset=dataset)\n",
    "model = model_factory.build(query_model=query_model, item_model=item_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerTrainer:\n",
    "    def __init__(self, \n",
    "                 dataset: TwoTowerDataset, \n",
    "                 model: TwoTowerModel) -> None:\n",
    "        self._dataset = dataset\n",
    "        self._model = model\n",
    "\n",
    "    def train(self, train_ds, val_ds):\n",
    "        self._initialize_models(train_ds)\n",
    "\n",
    "        # Define an optimizer using AdamW with a learning rate of 0.01\n",
    "        optimizer = tf.keras.optimizers.AdamW(\n",
    "            weight_decay=settings.TT_WEIGHT_DECAY,\n",
    "            learning_rate=settings.TT_LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        # Compile the model using the specified optimizer\n",
    "        self._model.compile(optimizer=optimizer)\n",
    "\n",
    "        # Start training\n",
    "        history = self._model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=settings.TT_EPOCHS,\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def _initialize_models(self, train_ds):\n",
    "        # Initialize age normalization layer.\n",
    "        self._model.query_model.normalized_age.adapt(train_ds.map(lambda x: x[\"age\"]))\n",
    "        self._model.item_model.normalized_year.adapt(train_ds.map(lambda x: x[\"year_of_publication\"]))\n",
    "\n",
    "        # Initialize model with inputs.\n",
    "        query_df = self._dataset.properties[\"query_df\"]\n",
    "        query_ds = self._dataset.df_to_ds(query_df).batch(1)\n",
    "        self._model.query_model(next(iter(query_ds)))\n",
    "\n",
    "        item_df = self._dataset.properties[\"item_df\"]\n",
    "        item_ds = self._dataset.df_to_ds(item_df).batch(1)\n",
    "        self._model.item_model(next(iter(item_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TwoTowerTrainer(dataset=dataset, model=model)\n",
    "history = trainer.train(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "# Training loss subplot\n",
    "ax1.plot(history.history[\"loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "ax1.set_title(\"Training Loss Over Time\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Validation loss subplot\n",
    "ax2.plot(history.history[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
    "ax2.set_title(\"Validation Loss Over Time\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "# plt.show() # Uncomment to show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import hopsworks\n",
    "import os\n",
    "from hsml.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopsworksQueryModel:\n",
    "    deployment_name = \"query\"\n",
    "\n",
    "    def __init__(self, model: QueryTower) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    def save_to_local(self, \n",
    "                      output_path: str = \"query_model\") -> str:\n",
    "        # Define the input specifications for the instances\n",
    "        instances_spec = {\n",
    "            \"user_id\": tf.TensorSpec(\n",
    "                shape=(None,), dtype=tf.string, name=\"user_id\"\n",
    "            ),  # Specification for customer IDs\n",
    "            \"age\": tf.TensorSpec(\n",
    "                shape=(None,), dtype=tf.float32, name=\"age\"\n",
    "            ),  # Specification for age\n",
    "        }\n",
    "\n",
    "        query_module_module = QueryModelModule(model=self.model)\n",
    "        # Get the concrete function for the query_model's compute_emb function using the specified input signatures\n",
    "        inference_signatures = (\n",
    "            query_module_module.compute_embedding.get_concrete_function(instances_spec)\n",
    "        )\n",
    "\n",
    "        # Save the query_model along with the concrete function signatures\n",
    "        tf.saved_model.save(\n",
    "            self.model,  # The model to save\n",
    "            output_path,  # Path to save the model\n",
    "            signatures=inference_signatures,  # Concrete function signatures to include\n",
    "        )\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    def register(self, \n",
    "                 mr, \n",
    "                 feature_view, \n",
    "                 query_df) -> None:\n",
    "        local_model_path = self.save_to_local()\n",
    "\n",
    "        # Sample a query example from the query DataFrame\n",
    "        query_example = query_df.sample().to_dict(\"records\") # [{'user_id': '141902', 'age': 51.0}]\n",
    "\n",
    "        # Create a tensorflow model for the query_model in the Model Registry\n",
    "        mr_query_model = mr.tensorflow.create_model(\n",
    "            name=\"query_model\",  # Name of the model\n",
    "            description=\"Model that generates query embeddings from user features\",  # Description of the model\n",
    "            input_example=query_example,  # Example input for the model\n",
    "            feature_view=feature_view,\n",
    "        )\n",
    "\n",
    "        # Save the query_model to the Model Registry\n",
    "        mr_query_model.save(local_model_path)  # Path to get the model\n",
    "\n",
    "    @classmethod\n",
    "    def deploy(cls, \n",
    "               ranking_model_type: Literal[\"ranking\", \"llmranking\"] = \"ranking\"):\n",
    "        # Prepare secrets used in the deployment\n",
    "        project = hopsworks.login()\n",
    "        cls._prepare_secrets(ranking_model_type)\n",
    "\n",
    "        mr = project.get_model_registry()\n",
    "        dataset_api = project.get_dataset_api()\n",
    "\n",
    "        # Retrieve the 'query_model' from the Model Registry\n",
    "        query_model = mr.get_model(\n",
    "            name=\"query_model\",\n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        # # Query-Transformer\n",
    "        # # Query-Transformer\n",
    "        # # Query-Transformer\n",
    "        # # Copy transformer file into Hopsworks File System\n",
    "        # uploaded_file_path = dataset_api.upload(\n",
    "        #     str(settings.RECSYS_DIR / \"inference\" / \"query_transformer.py\"),\n",
    "        #     \"Models\",\n",
    "        #     overwrite=True,\n",
    "        # )\n",
    "\n",
    "        # # Construct the path to the uploaded script\n",
    "        # transformer_script_path = os.path.join(\n",
    "        #     \"/Projects\",\n",
    "        #     project.name,\n",
    "        #     uploaded_file_path,\n",
    "        # )\n",
    "\n",
    "        # query_model_transformer = Transformer(\n",
    "        #     script_file=transformer_script_path,\n",
    "        #     resources={\"num_instances\": 0},\n",
    "        # )\n",
    "\n",
    "        # # Deploy the query model\n",
    "        # query_model_deployment = query_model.deploy(\n",
    "        #     name=cls.deployment_name,\n",
    "        #     description=\"Deployment that generates query embeddings from customer features using the query model\",\n",
    "        #     resources={\"num_instances\": 0},\n",
    "        #     transformer=query_model_transformer,\n",
    "        # )\n",
    "\n",
    "        # return query_model_deployment\n",
    "\n",
    "        # # Query-Transformer\n",
    "        # # Query-Transformer\n",
    "        # # Query-Transformer\n",
    "    \n",
    "    @classmethod\n",
    "    def _prepare_secrets(cls, \n",
    "                         ranking_model_type: Literal[\"ranking\", \"llmranking\"]):\n",
    "        project = hopsworks.login(\n",
    "            hostname_verification=False,\n",
    "            api_key_value=settings.HOPSWORKS_API_KEY.get_secret_value(),     \n",
    "        )\n",
    "        secrets_api = hopsworks.get_secrets_api()\n",
    "        secrets = secrets_api.get_secrets()\n",
    "        existing_secret_keys = [secret.name for secret in secrets]\n",
    "        if \"RANKING_MODEL_TYPE\" in existing_secret_keys:\n",
    "            secrets_api._delete(name=\"RANKING_MODEL_TYPE\")\n",
    "\n",
    "        secrets_api.create_secret(\n",
    "            \"RANKING_MODEL_TYPE\",\n",
    "            ranking_model_type,\n",
    "            project=project.name,\n",
    "        )\n",
    "\n",
    "\n",
    "class QueryModelModule(tf.Module):\n",
    "    def __init__(self, \n",
    "                 model: QueryTower) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function()\n",
    "    def compute_embedding(self, instances):\n",
    "        query_embedding = self.model(instances)\n",
    "\n",
    "        return {\n",
    "            \"customer_id\": instances[\"user_id\"],\n",
    "            # \"month_sin\": instances[\"month_sin\"],\n",
    "            # \"month_cos\": instances[\"month_cos\"],\n",
    "            \"query_emb\": query_embedding,\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model = HopsworksQueryModel(\n",
    "    model=model.query_model\n",
    ")\n",
    "query_model.register(\n",
    "    mr=mr,\n",
    "    feature_view=feature_view,\n",
    "    query_df=dataset.properties[\"query_df\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.properties[\"query_df\"].sample().to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopsworksCandidateModel:\n",
    "    def __init__(self, \n",
    "                 model: ItemTower):\n",
    "        self.model = model\n",
    "\n",
    "    def save_to_local(self, output_path: str = \"candidate_model\") -> str:\n",
    "        tf.saved_model.save(\n",
    "            self.model,  # The model to save\n",
    "            output_path,  # Path to save the model\n",
    "        )\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    def register(self, \n",
    "                 mr, \n",
    "                 feature_view, \n",
    "                 item_df):\n",
    "        local_model_path = self.save_to_local()\n",
    "\n",
    "        # Sample a candidate example from the item DataFrame\n",
    "        candidate_example = item_df.sample().to_dict(\"records\")\n",
    "\n",
    "        # Create a tensorflow model for the candidate_model in the Model Registry\n",
    "        mr_candidate_model = mr.tensorflow.create_model(\n",
    "            name=\"candidate_model\",  # Name of the model\n",
    "            description=\"Model that generates candidate embeddings from item features\",  # Description of the model\n",
    "            input_example=candidate_example,  # Example input for the model\n",
    "            feature_view=feature_view,\n",
    "        )\n",
    "\n",
    "        # Save the candidate_model to the Model Registry\n",
    "        mr_candidate_model.save(local_model_path)  # Path to save the model\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, \n",
    "                 mr) -> tuple[ItemTower, dict]:\n",
    "        models = mr.get_models(name=\"candidate_model\")\n",
    "        if len(models) == 0:\n",
    "            raise RuntimeError(\n",
    "                \"No 'candidate_model' found in Hopsworks model registry.\"\n",
    "            )\n",
    "        latest_model = max(models, key=lambda m: m.version)\n",
    "\n",
    "        logger.info(f\"Downloading 'candidate_model' version {latest_model.version}\")\n",
    "        model_path = latest_model.download()\n",
    "\n",
    "        # load downloaded model\n",
    "        candidate_model = tf.saved_model.load(model_path)\n",
    "\n",
    "        candidate_features = [\n",
    "            *candidate_model.signatures[\"serving_default\"]\n",
    "            .structured_input_signature[-1]\n",
    "            .keys()\n",
    "        ]\n",
    "        return candidate_model, candidate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_model = HopsworksCandidateModel(\n",
    "    model=model.item_model\n",
    ")\n",
    "item_model.register(\n",
    "    mr=mr,\n",
    "    feature_view=feature_view,\n",
    "    item_df=dataset.properties[\"item_df\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
